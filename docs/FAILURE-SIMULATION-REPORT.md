# ØªÙ‚Ø±ÙŠØ± Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„ÙØ´Ù„ ÙˆØ§Ù„ØªØ¹Ø§ÙÙŠ
# Failure Simulation & Recovery Report

## Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© | Overview
This document details the failure scenarios tested in the SRE Assignment system and how Kubernetes handled each failure situation.

## Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ù†ÙØ°Ø© | Implemented Scenarios

### Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ø£ÙˆÙ„: ÙØ´Ù„ Ø§Ù„Ø¨ÙˆØ¯ | Scenario 1: Pod Failure

**Ø§Ù„ÙˆØµÙ | Description:**
Ù…Ø­Ø§ÙƒØ§Ø© ÙØ´Ù„ ÙÙŠ Ø¨ÙˆØ¯ Ø®Ø¯Ù…Ø© API Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
Simulation of API service pod failure

**Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ° | Implementation Steps:**
```bash
# 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙˆØ¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ© | Check current pods
kubectl get pods -n production -l app=api-service

# 2. Ø­Ø°Ù Ø¨ÙˆØ¯ ÙˆØ§Ø­Ø¯ | Delete one pod
kubectl delete pod <pod-name> -n production

# 3. Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªØ¹Ø§ÙÙŠ | Monitor recovery
kubectl get pods -n production -l app=api-service -w
```

**Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±ØµÙˆØ¯Ø© | Observed Results:**
- â±ï¸ **ÙˆÙ‚Øª Ø§Ù„Ø§ÙƒØªØ´Ø§Ù | Detection Time:** ÙÙˆØ±ÙŠ (Ø£Ù‚Ù„ Ù…Ù† 10 Ø«ÙˆØ§Ù†ÙŠ) | Immediate (<10 seconds)
- ğŸ”„ **ÙˆÙ‚Øª Ø§Ù„ØªØ¹Ø§ÙÙŠ | Recovery Time:** 30-45 Ø«Ø§Ù†ÙŠØ© | 30-45 seconds  
- ğŸ“Š **Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…Ø±ØµÙˆØ¯ | Observed Behavior:**
  - Kubernetes automatically detected the pod failure
  - ReplicaSet immediately scheduled a new pod
  - Service continued to route traffic to healthy pods
  - No service downtime experienced
  - HPA metrics were updated correctly

**Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ù…Ø±ØµÙˆØ¯Ø© | Logged Events:**
```
LAST SEEN   TYPE     REASON      OBJECT                MESSAGE
0s          Normal   Killing     pod/api-service-xxx   Stopping container api
5s          Normal   Scheduled   pod/api-service-yyy   Successfully assigned production/api-service-yyy to minikube
10s         Normal   Pulling     pod/api-service-yyy   Pulling image "api-service:1.0.0"
15s         Normal   Pulled      pod/api-service-yyy   Successfully pulled image
20s         Normal   Created     pod/api-service-yyy   Created container api
25s         Normal   Started     pod/api-service-yyy   Started container api
```

### Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø­Ù…ÙˆÙ„Ø© | Scenario 2: Load Spike

**Ø§Ù„ÙˆØµÙ | Description:**
Ù…Ø­Ø§ÙƒØ§Ø© Ø²ÙŠØ§Ø¯Ø© Ù…ÙØ§Ø¬Ø¦Ø© ÙÙŠ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø®Ø¯Ù…Ø© Ø§Ù„ØµÙˆØ±
Simulation of sudden load increase on image service

**Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ° | Implementation Steps:**
```bash
# 1. ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø­Ù…ÙˆÙ„Ø© | Run load test
./scripts/test-runner.sh

# 2. Ù…Ø±Ø§Ù‚Ø¨Ø© HPA | Monitor HPA
kubectl get hpa -n production -w

# 3. Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¨ÙˆØ¯Ø§Øª | Monitor pods
kubectl get pods -n production -l app=image-service -w
```

**Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±ØµÙˆØ¯Ø© | Observed Results:**
- ğŸ“ˆ **Ø§Ø³ØªØ¬Ø§Ø¨Ø© HPA | HPA Response:** ØªÙ… Ø§Ù„ØªÙˆØ³Ø¹ Ù…Ù† 2 Ø¥Ù„Ù‰ 4 Ø¨ÙˆØ¯Ø§Øª | Scaled from 2 to 4 pods
- â±ï¸ **ÙˆÙ‚Øª Ø§Ù„ØªÙˆØ³Ø¹ | Scale-up Time:** 2-3 Ø¯Ù‚Ø§Ø¦Ù‚ | 2-3 minutes
- ğŸ’¾ **Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© | Memory Usage:** Ø§Ø±ØªÙØ¹ Ù…Ù† 45% Ø¥Ù„Ù‰ 75% | Rose from 45% to 75%
- ğŸ”„ **Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ | Auto-scaling:** Ù†Ø¬Ø­ ÙÙŠ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ù…ÙˆÙ„Ø© | Successfully managed load

### Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ø«Ø§Ù„Ø«: ÙØ´Ù„ Ø§Ù„Ø´Ø¨ÙƒØ© | Scenario 3: Network Failure

**Ø§Ù„ÙˆØµÙ | Description:**
Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø²Ù„ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø¹Ø¨Ø± Network Policies
Testing service isolation via Network Policies

**Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ° | Implementation Steps:**
```bash
# 1. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§ØªØµØ§Ù„ ØºÙŠØ± Ù…ØµØ±Ø­ Ø¨Ù‡ | Attempt unauthorized connection
kubectl exec -it <pod-name> -n production -- curl <forbidden-service>

# 2. Ù…Ø±Ø§Ù‚Ø¨Ø© Network Policies | Monitor Network Policies
kubectl describe networkpolicy -n production
```

**Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±ØµÙˆØ¯Ø© | Observed Results:**
- ğŸ›¡ï¸ **Ø§Ù„Ø­Ù…Ø§ÙŠØ© ÙØ¹Ø§Ù„Ø© | Security Effective:** ØªÙ… Ù…Ù†Ø¹ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØµØ±Ø­ Ø¨Ù‡Ø§ | Unauthorized connections blocked
- âœ… **Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…ØµØ±Ø­ Ø¨Ù‡Ø§ | Authorized Connections:** ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ | Working normally
- ğŸ“Š **Ù„Ø§ ØªØ£Ø«ÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡ | No Performance Impact:** Network policies didn't affect legitimate traffic

### Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ø±Ø§Ø¨Ø¹: ÙØ´Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª | Scenario 4: Database Connection Failure

**Ø§Ù„ÙˆØµÙ | Description:**
Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù†Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
Simulation of external database connection failure

**Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ° | Implementation Steps:**
```bash
# 1. ØªØ¹Ø¯ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ù„ØªÙƒÙˆÙ† Ø®Ø§Ø·Ø¦Ø© | Modify connection to be incorrect
kubectl patch secret database-credentials -n production -p '{"data":{"postgres-password":"invalid"}}'

# 2. Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØ¯Ø§Øª | Restart pods
kubectl rollout restart deployment/api-service -n production

# 3. Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø­Ø§Ù„Ø© | Monitor status
kubectl logs -f deployment/api-service -n production
```

**Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±ØµÙˆØ¯Ø© | Observed Results:**
- ğŸš¨ **Ø§Ù„ÙƒØ´Ù Ø§Ù„Ø³Ø±ÙŠØ¹ | Quick Detection:** Readiness probes detected failure immediately
- ğŸ”„ **Ù…Ù†Ø¹ Ø§Ù„ØªØ±Ø§ÙÙŠÙƒ | Traffic Prevention:** Unhealthy pods removed from service endpoints
- ğŸ“Š **Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª | Alerts:** Prometheus alerts fired correctly
- ğŸ›¡ï¸ **Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ù„ØªØªØ§Ù„ÙŠ | Cascade Protection:** Other services remained operational

## Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª ÙˆØ§Ù„Ø±ØµØ¯ | Alerts & Monitoring

### Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ù…ÙØ¹Ù„Ø© | Active Alerts

**ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø±Ø¬Ø© | Critical Alerts:**
- ğŸ”´ **ServiceDown:** Ø¹Ù†Ø¯ ØªÙˆÙ‚Ù Ø§Ù„Ø®Ø¯Ù…Ø© | When service is down
- ğŸ”´ **PodCrashLooping:** Ø¹Ù†Ø¯ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØ¯ Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© | When pod is restarting frequently

**ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„ØªØ­Ø°ÙŠØ± | Warning Alerts:**
- ğŸŸ¡ **HighCPUUsage:** Ø¹Ù†Ø¯ ØªØ¬Ø§ÙˆØ² 80% | When CPU > 80%
- ğŸŸ¡ **HighMemoryUsage:** Ø¹Ù†Ø¯ ØªØ¬Ø§ÙˆØ² 80% | When Memory > 80%
- ğŸŸ¡ **PodNotReady:** Ø¹Ù†Ø¯ Ø¹Ø¯Ù… Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ø¨ÙˆØ¯ | When pod not ready

### Ø£Ù…Ø«Ù„Ø© Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª | Alert Examples

```yaml
Alert: ServiceDown
Labels: {alertname="ServiceDown", job="api-service", severity="critical"}
Annotations:
  summary: Service is down
  description: Service api-service is not responding
```

## Ø§Ù„Ø¯Ø±ÙˆØ³ Ø§Ù„Ù…Ø³ØªÙØ§Ø¯Ø© | Lessons Learned

### Ù…Ø§ Ù†Ø¬Ø­ | What Worked Well:
1. **Ø§Ù„ØªØ¹Ø§ÙÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ | Auto-recovery:** Kubernetes quickly replaced failed pods
2. **Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ | Auto-scaling:** HPA responded appropriately to load
3. **Ø§Ù„Ø¹Ø²Ù„ Ø§Ù„Ø¢Ù…Ù† | Secure Isolation:** Network policies effectively blocked unauthorized access
4. **Ø§Ù„Ø±ØµØ¯ Ø§Ù„Ø´Ø§Ù…Ù„ | Comprehensive Monitoring:** All failures were detected and alerted

### Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† | Areas for Improvement:
1. **ÙˆÙ‚Øª Ø§Ù„ØªÙˆØ³Ø¹ | Scale-up Time:** ÙŠÙ…ÙƒÙ† ØªÙ‚Ù„ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© | Response time could be reduced
2. **ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª | Alert Aggregation:** Ø¯Ù…Ø¬ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·Ø© | Combine related alerts
3. **Ø§Ù„ØªØ¹Ø§ÙÙŠ Ù…Ù† ÙØ´Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª | DB Failure Recovery:** Ø¥Ø¶Ø§ÙØ© Ø¢Ù„ÙŠØ© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© | Add retry mechanism

## Ø§Ù„ØªÙˆØµÙŠØ§Øª | Recommendations

### ØªÙˆØµÙŠØ§Øª Ù‚ØµÙŠØ±Ø© Ø§Ù„Ù…Ø¯Ù‰ | Short-term:
- ØªÙ‚Ù„ÙŠÙ„ ÙØªØ±Ø© readinessProbe Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£Ø³Ø±Ø¹ | Reduce readinessProbe period for faster response
- Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ© | Add more custom metrics
- ØªØ­Ø³ÙŠÙ† Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª | Improve alert messages

### ØªÙˆØµÙŠØ§Øª Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰ | Long-term:
- ØªØ·Ø¨ÙŠÙ‚ Chaos Engineering Ø¨Ø´ÙƒÙ„ Ù…Ù†ØªØ¸Ù… | Implement regular Chaos Engineering
- Ø¥Ø¶Ø§ÙØ© Multi-cluster deployment | Add multi-cluster deployment
- ØªØ·Ø¨ÙŠÙ‚ Circuit Breaker pattern | Implement Circuit Breaker pattern

## Ø§Ù„Ø®Ù„Ø§ØµØ© | Conclusion

Ù†Ø¸Ø§Ù… SRE Assignment Ø£Ø¸Ù‡Ø± Ù…Ø±ÙˆÙ†Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„ÙØ´Ù„. Kubernetes Ùˆ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…ØµØ§Ø­Ø¨Ø© Ù†Ø¬Ø­Øª ÙÙŠ:
- Ø§Ù„ÙƒØ´Ù Ø§Ù„Ø³Ø±ÙŠØ¹ Ø¹Ù† Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
- Ø§Ù„ØªØ¹Ø§ÙÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ù† Ø§Ù„ÙØ´Ù„  
- Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ ØªÙˆÙØ± Ø§Ù„Ø®Ø¯Ù…Ø©
- Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©

The SRE Assignment system demonstrated high resilience in handling various types of failures. Kubernetes and associated tools successfully:
- Quickly detected issues
- Automatically recovered from failures
- Maintained service availability  
- Sent appropriate alerts

Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙŠÙˆØ¶Ø­ Ø£Ù† Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¥Ù†ØªØ§Ø¬ Ù…Ø¹ Ù‚Ø¯Ø±Ø§Øª Ù…ÙˆØ«ÙˆÙ‚Ø© Ù„Ù„ØªØ¹Ø§ÙÙŠ Ù…Ù† Ø§Ù„ÙØ´Ù„.
This report demonstrates that the system is production-ready with reliable failure recovery capabilities.